# LangChain

## 00. LLM 기초

<details>
<summary>Contents</summary>
<div markdown="1">

## Algorithm vs ML

- 룰 베이스, 엔지니어가 작성 vs 방대한 데이터를 기반으로 알고리즘을 학습

> 소프트웨어 엔지니어의 역할 알고리즘 작성 -> 알고리즘을 학습하는 모델 개발

## ML vs LLM

- 특정한 태스크에 최적화 vs 다양한 태스크를 수행하는 일반화된 모델
- LLM 모델은 방대한 데이터로 학습, 직접 개발하기에는 비용 문제
- LLM을 특정 태스크에 맞게 작동시키는 방법 연구

## LLM 기초

**ANN 기반의 태스크들의 원리**

> 1. 입력을 바탕으로 고차원의 잠재 벡터(Latent Vector)를 생성
> 2. 잠재 벡터를 어떻게 표현하는지에 따라 텍스트, 이미지를 생성
> 3. 잠재 벡터에 약간의 노이즈를 추가하여 다양한 결과를 생성
> 4. 이전의 RNN, CNN 기반 구조가 Transformer 기반으로 변경됨

- 거대 언어 모델(LLM)

  - 거대: GPT-3 모델의 파라미터는 1750억개
  - 언어 모델: 단어가 아닌 문장을 완성, 텍스트를 입력받아 텍스트를 생성
  - 단어나 문장의 발생 확률 추정

- 프롬프트(Prompt)

  - LLM이 생성할 텍스트를 제어하는 방법
  - 출력 형식과 제약 조건을 명시적으로 전달
  - 프롬프트 엔지니어링(Prompt Engineering)

- 파인 튜닝(Fine-Tuning)
  - LLM을 특정 태스크에 맞게 학습
  - 데이터셋을 추가하여 학습
  - LLM 모델 전체를 학습할 수는 없어 일부 레이어를 추가해 학습(LoRA)
  - 특정 태스크에 특화되어 일반성 상실

## 프롬프트 엔지니어링(Prompt Engineering)

- 프롬프트를 최적화하여 LLM의 성능을 높이는 방법
- 제로샷 프롬프트

  - 단순 작업 지시

- 퓨샷 프롬프트

  - 몇 가지의 예시를 통해 LLM이 생성할 텍스트를 제어하는 방법

- 사고의 연쇄(Chain of Thought)

  - LLM이 문제를 해결하는 과정을 명시적으로 전달

    ```
    Q. 100 이하의 소수를 구하시오.

    A: 생각해봅시다. 에라토스테네스의 체 알고리즘의 순서는 다음과 같습니다.

    1. 2부터 100까지의 모든 정수를 나열합니다.
    2. 2는 소수이므로, 2의 배수(2를 제외한 4, 6, 8, ..., 100)를 모두 지웁니다.
    3. 남아있는 수 중에서 다음 소수(3)를 찾습니다. 3의 배수(3을 제외한 6, 9, 12, ..., 99)를 모두 지웁니다.
    4. 그 다음 남아있는 수(5)에 대해 5의 배수(5를 제외한 10, 15, 20, ..., 100)를 모두 지웁니다.
    5. 이 과정을 100 이하의 수에 대해 반복합니다. 이미 지워진 수는 건너뜁니다.
    6. 마지막까지 남아있는 수들이 모두 소수입니다.

    이렇게 하면 100 이하의 모든 소수를 효율적으로 구할 수 있습니다.
    ```

- RAG(Retrieval Augmented Generation)

  - LLM이 데이터를 활용하여 응답 생성
  - 데이터베이스, 파일, 웹사이트 등 다양한 데이터를 활용
  - 데이터를 활용하여 응답 생성

- 툴 호출

  - LLM이 사용할 수 있는 도구를 정의
  - 도구 호출 결과를 사용하여 응답 생성
  - MCP(Model Context Protocol)

## LangChain

- 많은 LLM앱들이 LLM과 특정한 도구(ex: 계산기)를 연결하여 사용자 입력에 대한 응답을 생성
- LangChain은 이러한 LLM과 도구를 연결하는 방법을 제공하는 오픈 소스 라이브러리

</div>
</details>

## 01. 랭체인 기본 LLM 사용 방법

<details>
<summary>Contents</summary>
<div markdown="1">

</div>
</details>
